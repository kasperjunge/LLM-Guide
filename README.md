# LLM-Guide ğŸš€

Welcome to LLM-Guide, a resource for training, hosting and using large language models (LLMs). This repository is designed to provide a central hub for researchers, developers, and enthusiasts who are interested in the latest developments in everything LLMs.

## Open-Source Models á¨
- [Bloom ğŸŒ¸: BigScience Large Open-science Open-access Multilingual Language Model](https://huggingface.co/bigscience/bloom)
- [OPT-175B: Democratizing access to large-scale language](https://forms.gle/BDB2i44QwCr2mCJN6)
- [GALACTICA 120B: trained on a large-scale scientific corpus](https://huggingface.co/facebook/galactica-120b)
- [LLaMA: Open and Efficient Foundation Language Models](https://github.com/facebookresearch/llama)

## Open Sources Projects ğŸ‘©â€ğŸ’»
  - [LangChain: âš¡ Building applications with LLMs through composability âš¡ï¸](https://github.com/hwchase17/langchain)
  - [Alpa: Training and serving large-scale neural networks](https://github.com/alpa-projects/alpa)
  - [Petals: ğŸŒ¸ Run 100B+ language models at home, BitTorrent-style.](https://github.com/bigscience-workshop/petals)
  - [Open Assistant: Open source ChatGPT-like model](https://open-assistant.io)
  - [Together: A decentralized cloud for artificial intelligence.](https://www.together.xyz/)
  - [Chroma: the open source embedding database](https://github.com/chroma-core/chroma)

## LLM Providers ğŸ’»
- [OpenAI](https://openai.com/)
- [Cohere](https://cohere.ai/)
- [AI21labs](https://www.ai21.com/)
- [GooseAI](https://goose.ai/)
- [DeepInfra](https://deepinfra.com/)

## Papers ğŸ“œ
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [OPT: Open Pre-trained Transformer Language Models (OPT-175B](https://arxiv.org/abs/2205.01068)
- [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
- [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)

## Tutorials ğŸ“
### Video ğŸ¥
- [LangChain Tutorial](https://youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)
- [LangChain for Gen AI and LLMs](https://youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)
- [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY)
### Notebook ğŸª
- [langchain-tutorials](https://github.com/gkamradt/langchain-tutorials)

## Datasets ğŸ’¾
- [Anthropic: HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [OpenAI: summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)

## People to Follow ğŸ’
- James Briggs: [Youtube](https://www.youtube.com/@jamesbriggs), [Twitter](https://twitter.com/jamescalam), [LinkedIn](https://www.linkedin.com/in/jamescalam/), [GitHub](https://github.com/jamescalam)
- Elvis Saravia: [Youtube](https://www.youtube.com/@elvissaravia), [Twitter](https://twitter.com/omarsar0), [LinkedIn](https://www.linkedin.com/in/omarsar/), [GitHub](https://github.com/dair-ai)
- [Data Independent](https://www.youtube.com/@DataIndependent)

