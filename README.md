# LLM-Guide üöÄ

Welcome to LLM-Guide, a resource for training, hosting and developing with large language models (LLMs).

### APIs
| Model | Provider | Price | Size | Type | Link |
| --- | --- | --- | --- | --- | --- |
| Luminous-supreme| Aleph Alpha  | $0.0.1750 / 1K tokens | 70B | Text Generation | [link](https://www.aleph-alpha.com/pricing) |
| Luminous-extended| Aleph Alpha  | $0.0450 / 1K tokens | 30B | Text Generation | [link](https://www.aleph-alpha.com/pricing) |
| Luminous-base | Aleph Alpha  | $0.0300 / 1K tokens | 13B | Text Generation | [link](https://www.aleph-alpha.com/pricing) |
| j2-jumbo | AI21 | $0.0150 / 1K tokens | 178B | Text Generation | [link](https://www.ai21.com/studio/pricing) |
| j2-grande | AI21 | $0.0100 / 1K tokens | 17B | Text Generation | [link](https://www.ai21.com/studio/pricing) |
| j2-large | AI21 | $0.00300 / 1K tokens | 7.5B | Text Generation |[link]([https://www.ai21.com/studio/pricing) |
| default | Cohere | $2.5 per 1000 Generation Units* | - | Text Generation | [link](https://cohere.ai/pricing) |
| gpt-3.5-turbo	| OpenAI  | $0.0020 / 1K tokens | 175B | Text Generation | [link](https://openai.com/pricing) |
| text-davinci-003 | OpenAI | $0.0200 / 1K tokens | 175B | Text Generation | [link](https://openai.com/pricing) |
| text-curie-001 | OpenAI | $0.0020 / 1K tokens | 6.7B | Text Generation | [link](https://openai.com/pricing) |
| text-babbage-001 | OpenAI | $0.0005 / 1K tokens | 1.3B | Text Generation | [link](https://openai.com/pricing) |
| text-ada-001 | OpenAI | $0.0004 / 1K tokens | - | Text Generation | [link](https://openai.com/pricing) |
| code-davinci-002 | OpenAI | ? / 1K tokens | - | Code Generation | [link](https://openai.com/pricing) |
| code-cushman-001 | OpenAI | ? / 1K tokens | 12B | Code Generation | [link](https://openai.com/pricing) |
| text-embedding-ada-002 | OpenAI | $0.0004 / 1K tokens | - | Text Embedding | [link](https://openai.com/pricing) |
| default | Cohere | $1.0 per 1000 Embeddings | - | Text Embedding | [link](https://cohere.ai/pricing) |

## Open-Source Models ·ç®
- [BLOOM üå∏: BigScience Large Open-science Open-access Multilingual Language Model](https://huggingface.co/bigscience/bloom)
- [BLOOMZ](https://huggingface.co/bigscience/bloomz)
- [OPT-175B: Democratizing access to large-scale language](https://forms.gle/BDB2i44QwCr2mCJN6)
- [GALACTICA 120B: trained on a large-scale scientific corpus](https://huggingface.co/facebook/galactica-120b)
- [LLaMA: Open and Efficient Foundation Language Models](https://github.com/facebookresearch/llama)
- [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)
- [GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)
- [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B)
- [GLM-130B](https://github.com/THUDM/GLM-130B)
- [YaLM 100B](https://github.com/yandex/YaLM-100B)
- [UL2 20B](https://huggingface.co/google/ul2)
- [H3 2.7B](https://huggingface.co/danfu09/H3-2.7B)

## Open Sources Projects üë©‚Äçüíª
  - [LangChain: ‚ö° Building applications with LLMs through composability ‚ö°Ô∏è](https://github.com/hwchase17/langchain)
  - [Petals: üå∏ Run 100B+ language models at home, BitTorrent-style.](https://github.com/bigscience-workshop/petals)
  - [Open Assistant: Open source ChatGPT-like model](https://open-assistant.io)
  - [Together: A decentralized cloud for artificial intelligence.](https://www.together.xyz/)
  - [Runhouse](https://github.com/run-house/runhouse)
  - [Chroma: the open source embedding database](https://github.com/chroma-core/chroma)
  - [ChatLLaMA: LLaMA-based ChatGPT Training Process](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama)
  - [OpenChatKit](https://github.com/togethercomputer/OpenChatKit)
  - [HELM: Holistic Evaluation of Language Models](https://github.com/stanford-crfm/helm)
  - [Guardrails: Guardrails is an open-source Python package for specifying structure and type, validating and correcting the outputs of large language models (LLMs).](https://github.com/shreyar/guardrails)

## LLM Providers üíª
- [OpenAI](https://openai.com/)
- [Cohere](https://cohere.ai/)
- [AI21labs](https://www.ai21.com/)
- [GooseAI](https://goose.ai/)
- [DeepInfra](https://deepinfra.com/)
- [ForefronAI](https://www.forefront.ai/)
- [NLP Cloud](https://nlpcloud.com/)

## LLM Training Frameworks and Tools
- [Alpa: Training and serving large-scale neural networks](https://github.com/alpa-projects/alpa)
- [DeepSpeed (Microsoft)](https://github.com/microsoft/DeepSpeed)
- [Composer (MosaicML)](https://github.com/mosaicml/composer)
- [Colassal-AI](https://github.com/hpcaitech/ColossalAI)
- [BMTrain](https://github.com/OpenBMB/BMTrain)
- [Flower](https://github.com/adap/flower)
- [Adap](https://www.adap.com/en)

## Papers üìú
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [OPT: Open Pre-trained Transformer Language Models (OPT-175B)](https://arxiv.org/abs/2205.01068)
- [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
- [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
- [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## LLM Guides
- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
- [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
- [Awesome ChatGPT](https://github.com/humanloop/awesome-chatgpt)
- [Using LLaMA with M1 Mac](https://dev.l1x.be/posts/2023/03/12/using-llama-with-m1-mac/)

## LLMOps Services
- [Human Loop](https://humanloop.com/)

## Tutorials üéì
### Video üé•
- [LangChain Tutorial](https://youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)
- [LangChain for Gen AI and LLMs](https://youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)
- [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY)
### Notebook ü™ê
- [langchain-tutorials](https://github.com/gkamradt/langchain-tutorials)

## Datasets üíæ
- [Anthropic: HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [OpenAI: summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)

## People to Follow üíé
- James Briggs: [Youtube](https://www.youtube.com/@jamesbriggs), [Twitter](https://twitter.com/jamescalam), [LinkedIn](https://www.linkedin.com/in/jamescalam/), [GitHub](https://github.com/jamescalam)
- Elvis Saravia: [Youtube](https://www.youtube.com/@elvissaravia), [Twitter](https://twitter.com/omarsar0), [LinkedIn](https://www.linkedin.com/in/omarsar/), [GitHub](https://github.com/dair-ai)
- [Data Independent](https://www.youtube.com/@DataIndependent)

