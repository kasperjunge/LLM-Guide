# LLM-Guide üöÄ

Welcome to LLM-Guide, a resource for training, hosting and developing with large language models (LLMs). This repository is designed to provide a central hub for researchers, developers, and enthusiasts who are interested in the latest developments in everything LLMs.

## Open-Source 
### Models ·ç®
- [Bloom üå∏: BigScience Large Open-science Open-access Multilingual Language Model](https://huggingface.co/bigscience/bloom)
- [OPT-175B: Democratizing access to large-scale language](https://forms.gle/BDB2i44QwCr2mCJN6)
- [GALACTICA 120B: trained on a large-scale scientific corpus](https://huggingface.co/facebook/galactica-120b)
- [LLaMA: Open and Efficient Foundation Language Models](https://github.com/facebookresearch/llama)
- [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)
- [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B)

### Open Sources Projects üë©‚Äçüíª
  - [LangChain: ‚ö° Building applications with LLMs through composability ‚ö°Ô∏è](https://github.com/hwchase17/langchain)
  - [Alpa: Training and serving large-scale neural networks](https://github.com/alpa-projects/alpa)
  - [Petals: üå∏ Run 100B+ language models at home, BitTorrent-style.](https://github.com/bigscience-workshop/petals)
  - [Open Assistant: Open source ChatGPT-like model](https://open-assistant.io)
  - [Together: A decentralized cloud for artificial intelligence.](https://www.together.xyz/)
  - [Runhouse](https://github.com/run-house/runhouse)
  - [Chroma: the open source embedding database](https://github.com/chroma-core/chroma)

## LLM Providers üíª
- [OpenAI](https://openai.com/)
- [Cohere](https://cohere.ai/)
- [AI21labs](https://www.ai21.com/)
- [GooseAI](https://goose.ai/)
- [DeepInfra](https://deepinfra.com/)
- [ForefronAI](https://www.forefront.ai/)
- [NLP Cloud](https://nlpcloud.com/)

## Papers üìú
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [OPT: Open Pre-trained Transformer Language Models (OPT-175B)](https://arxiv.org/abs/2205.01068)
- [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
- [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
- [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Tutorials üéì
### Video üé•
- [LangChain Tutorial](https://youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)
- [LangChain for Gen AI and LLMs](https://youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)
- [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY)
### Notebook ü™ê
- [langchain-tutorials](https://github.com/gkamradt/langchain-tutorials)

## Datasets üíæ
- [Anthropic: HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [OpenAI: summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)

## People to Follow üíé
- James Briggs: [Youtube](https://www.youtube.com/@jamesbriggs), [Twitter](https://twitter.com/jamescalam), [LinkedIn](https://www.linkedin.com/in/jamescalam/), [GitHub](https://github.com/jamescalam)
- Elvis Saravia: [Youtube](https://www.youtube.com/@elvissaravia), [Twitter](https://twitter.com/omarsar0), [LinkedIn](https://www.linkedin.com/in/omarsar/), [GitHub](https://github.com/dair-ai)
- [Data Independent](https://www.youtube.com/@DataIndependent)

